
- [On the scalability of diffusion-based text-to-image generation](https://arxiv.org/abs/2404.02883). H. Li, Y. Zou, Y. Wang, O. Majumder, Y. Xie, R. Manmatha, A. Swaminathan, Z. Tu, S. Ermon, S. Soatto, arXiv preprint (2024)
- [Video-llama: An instruction-tuned audio-visual language model for video understanding](https://arxiv.org/abs/2306.02858), H. Zhang, X. Li, L. Bing, arXiv preprint (2023)
- [Tifa: Accurate and interpretable text-to-image faithfulness evaluation with question answering](https://openaccess.thecvf.com/content/CVPR2023/html/Hu_TiFA_Accurate_and_Interpretable_Text-to-Image_Faithfulness_Evaluation_With_Question_Answering_CVPR_2023_paper.html). Hu, Y., Liu, B., Kasai, J., Wang, Y., Ostendorf, M., Krishna, R., Smith, N.A., IEEE/CVF International Conference on Computer Vision (2023)
- [Otterhd: A high-resolution multi-modality model](https://arxiv.org/abs/2311.04219). Li, B., Zhang, P., Yang, J., Zhang, Y., Pu, F., Liu, Z., arXiv preprint (2023)
- [Human preference score v2: A solid benchmark for evaluating human preferences of text-to-image synthesis](https://arxiv.org/abs/2306.09341). Chen, Z., Zhu, F., Zhao, R., Li, H., arXiv preprint (2023)
- [Minigpt-4: Enhancing vision-language understanding with advanced large language models](https://arxiv.org/abs/2304.10592). Zhu, D., Chen, J., Shen, X., Li, X., Elhoseiny, M., arXiv preprint (2023)
- [Agiqa-3k: An open database for ai-generated image quality assessment](https://ieeexplore.ieee.org/document/9797603). Li, C., Zhang, Z., Wu, H., Sun, W., Min, X., Liu, X., Zhai, G., Lin, W., IEEE Transactions on Circuits and Systems for Video Technology (2023)
- [Fake it to make it: Using synthetic data to remedy the data shortage in joint multimodal speech-and-gesture synthesis](https://arxiv.org/abs/2404.19622). Mehta, S., Deichler, A., Oâ€™Regan, J., Mo Ìˆell, B., Beskow, J., Henter, G.E., Alexanderson, S., arXiv preprint (2024)
- [Mini-gemini: Mining the potential of multi-modality vision language models](https://arxiv.org/abs/2403.18814). Li, Y., Zhang, Y., Wang, C., Zhong, Z., Chen, Y., Chu, R., Liu, S., Jia, J., arXiv preprint (2024)
- [Large multi-modality model assisted ai-generated image quality assessment](https://arxiv.org/abs/2404.17762). Wang, P., Sun, W., Zhang, Z., Jia, J., Jiang, Y., Zhang, Z., Min, X., Zhai, G., arXiv preprint (2024)
- [Visual-critic: Making lmms perceive visual quality like humans](https://arxiv.org/abs/2403.12806). Huang, Z., Zhang, Z., Lu, Y., Zha, Z.-J., Chen, Z., Guo, B., arXiv preprint (2024)
- [Make-it-real: Unleashing large multimodal modelâ€™s ability for painting 3d objects with realistic materials](https://arxiv.org/abs/2404.16829). Fang, Y., Sun, Z., Wu, T., Wang, J., Liu, Z., Wetzstein, G., Lin, D., arXiv preprint (2024)
- [A review of multi-modal large language and vision models](https://arxiv.org/abs/2404.01322). Carolan, K., Fennelly, L., Smeaton, A.F., arXiv preprint (2024)
- [Allava: Harnessing gpt4v-synthesized data for a lite vision-language model](https://arxiv.org/abs/2402.11684). Chen, G.H., Chen, S., Zhang, R., Chen, J., Wu, X., Zhang, Z., Chen, Z., Li, J., Wan, X., Wang, B., arXiv preprint (2024)
- [How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites](https://arxiv.org/abs/2404.16821). Chen, Z., Wang, W., Tian, H., Ye, S., Gao, Z., Cui, E., Tong, W., Hu, K., Luo, J., Ma, Z., et al., arXiv preprint (2024)
- [Aigiqa-20k: A large database for ai-generated image quality assessment](https://arxiv.org/abs/2404.03407). Li, C., Kou, T., Gao, Y., Cao, Y., Sun, W., Zhang, Z., Zhou, Y., Zhang, Z., Zhang, W., Wu, H., et al., arXiv preprint (2024)
- [Clip with quality captions: A strong pretraining for vision tasks](https://arxiv.org/abs/2405.08911). Vasu, P.K.A., Pouransari, H., Faghri, F., Tuzel, O., arXiv preprint (2024)
- [Do llms understand visual anomalies? uncovering llm capabilities in zero-shot anomaly detection](https://arxiv.org/abs/2404.09654). Zhu, J., Cai, S., Deng, F., Wu, J., arXiv preprint (2024)
- [Imagereward: Learning and evaluating human preferences for text-to-image generation](https://papers.nips.cc/paper/2024/file/2e90ed12c4e40b6421e20e55db94d36d-Paper.pdf). Xu, J., Liu, X., Wu, Y., Tong, Y., Li, Q., Ding, M., Tang, J., Dong, Y., Advances in Neural Information Processing Systems (2024)
- [mplug-owl2: Revolutionizing multi-modal large language model with modality collaboration](https://arxiv.org/abs/2311.04257). Ye, Q., Xu, H., Ye, J., Yan, M., Liu, H., Qian, Q., Zhang, J., Huang, F., Zhou, J., arXiv preprint (2023)
- [DeepInteraction++: Multi-Modality Interaction for Autonomous Driving](http://arxiv.org/abs/2408.05075)  
  Yang, Z., Song, N., Li, W., Zhu, X., Zhang, L., & Torr, P. H. S. (2024). *arXiv preprint*.  

  

- [Multimodal Fusion on Low-Quality Data: A Comprehensive Survey](http://arxiv.org/abs/2404.18947)  
  Zhang, Q., Wei, Y., Han, Z., Fu, H., Peng, X., Deng, C., Hu, Q., Xu, C., Wen, J., Hu, D., & Zhang, C. (2024). *arXiv preprint*.  


- [Deep Multimodal Data Fusion](https://dl.acm.org/doi/10.1145/3649447)  
  Zhao, F., Zhang, C., & Geng, B. (2024). *ACM Computing Surveys, 56*(9), 1â€“36. https://doi.org/10.1145/3649447  



- [MST-GAT: A Multimodal Spatial-Temporal Graph Attention Network for Time Series Anomaly Detection](http://arxiv.org/abs/2310.11169)  
  Ding, C., Sun, S., & Zhao, J. (2023). *Information Fusion, 89*, 527â€“536. https://doi.org/10.1016/j.inffus.2022.08.011  



- [Object Detection in Adverse Weather for Autonomous Vehicles Based on Sensor Fusion and Incremental Learning](https://ieeexplore.ieee.org/document/10715515/)  
  Su, H., Gao, H., Wang, X., Fang, X., Liu, Q., Huang, G., Li, X., & Cao, Q. (2024). *IEEE Transactions on Instrumentation and Measurement, 1â€“1.* https://doi.org/10.1109/TIM.2024.3472860  



- [Multi-Modal 3D Object Detection in Autonomous Driving: A Survey and Taxonomy](https://ieeexplore.ieee.org/document/10093116/)  
  Wang, L., Zhang, X., Song, Z., Bi, J., Zhang, G., Wei, H., Tang, L., Yang, L., Li, J., Jia, C., & Zhao, L. (2023). *IEEE Transactions on Intelligent Vehicles, 8*(7), 3781â€“3798. https://doi.org/10.1109/TIV.2023.3264658  



- [Eliminating Cross-Modal Conflicts in BEV Space for LiDAR-Camera 3D Object Detection](http://arxiv.org/abs/2403.07372)  
  Fu, J., Gao, C., Wang, Z., Yang, L., Wang, X., Mu, B., & Liu, S. (2024). *arXiv preprint*.

 - [Quality Assessment of Image Dataset for Autonomous Driving](https://ieeexplore.ieee.org/document/10355734/)  
  Li, X., Zhang, Y., Shi, Y., Zhu, H., Hu, J., & Peng, L. (2023). *In 2023 IEEE International Conference on Imaging Systems and Techniques (IST) (pp. 1â€“6). IEEE.* https://doi.org/10.1109/IST59124.2023.10355734  

  ðŸ”¹ Proposes a framework for evaluating the quality of image datasets used in autonomous driving, considering factors such as annotation accuracy and data diversity.




- [Explore the LiDAR-Camera Dynamic Adjustment Fusion for 3D Object Detection](http://arxiv.org/abs/2407.15334)  
  Yang, Y., Gao, X., Wang, T., Hao, X., Shi, Y., Tan, X., Ye, X., & Wang, J. (2024). *arXiv preprint*.  



