
- [On the scalability of diffusion-based text-to-image generation](https://arxiv.org/abs/2404.02883). H. Li, Y. Zou, Y. Wang, O. Majumder, Y. Xie, R. Manmatha, A. Swaminathan, Z. Tu, S. Ermon, S. Soatto, arXiv preprint (2024)
- [Video-llama: An instruction-tuned audio-visual language model for video understanding](https://arxiv.org/abs/2306.02858), H. Zhang, X. Li, L. Bing, arXiv preprint (2023)
- [Tifa: Accurate and interpretable text-to-image faithfulness evaluation with question answering](https://openaccess.thecvf.com/content/CVPR2023/html/Hu_TiFA_Accurate_and_Interpretable_Text-to-Image_Faithfulness_Evaluation_With_Question_Answering_CVPR_2023_paper.html). Hu, Y., Liu, B., Kasai, J., Wang, Y., Ostendorf, M., Krishna, R., Smith, N.A., IEEE/CVF International Conference on Computer Vision (2023)
- [Otterhd: A high-resolution multi-modality model](https://arxiv.org/abs/2311.04219). Li, B., Zhang, P., Yang, J., Zhang, Y., Pu, F., Liu, Z., arXiv preprint (2023)
- [Human preference score v2: A solid benchmark for evaluating human preferences of text-to-image synthesis](https://arxiv.org/abs/2306.09341). Chen, Z., Zhu, F., Zhao, R., Li, H., arXiv preprint (2023)
- [Minigpt-4: Enhancing vision-language understanding with advanced large language models](https://arxiv.org/abs/2304.10592). Zhu, D., Chen, J., Shen, X., Li, X., Elhoseiny, M., arXiv preprint (2023)
- [Agiqa-3k: An open database for ai-generated image quality assessment](https://ieeexplore.ieee.org/document/9797603). Li, C., Zhang, Z., Wu, H., Sun, W., Min, X., Liu, X., Zhai, G., Lin, W., IEEE Transactions on Circuits and Systems for Video Technology (2023)
- [Fake it to make it: Using synthetic data to remedy the data shortage in joint multimodal speech-and-gesture synthesis](https://arxiv.org/abs/2404.19622). Mehta, S., Deichler, A., O’Regan, J., Mo ̈ell, B., Beskow, J., Henter, G.E., Alexanderson, S., arXiv preprint (2024)
- [Mini-gemini: Mining the potential of multi-modality vision language models](https://arxiv.org/abs/2403.18814). Li, Y., Zhang, Y., Wang, C., Zhong, Z., Chen, Y., Chu, R., Liu, S., Jia, J., arXiv preprint (2024)
- [Large multi-modality model assisted ai-generated image quality assessment](https://arxiv.org/abs/2404.17762). Wang, P., Sun, W., Zhang, Z., Jia, J., Jiang, Y., Zhang, Z., Min, X., Zhai, G., arXiv preprint (2024)
- [Visual-critic: Making lmms perceive visual quality like humans](https://arxiv.org/abs/2403.12806). Huang, Z., Zhang, Z., Lu, Y., Zha, Z.-J., Chen, Z., Guo, B., arXiv preprint (2024)
- [Make-it-real: Unleashing large multimodal model’s ability for painting 3d objects with realistic materials](https://arxiv.org/abs/2404.16829). Fang, Y., Sun, Z., Wu, T., Wang, J., Liu, Z., Wetzstein, G., Lin, D., arXiv preprint (2024)
- [A review of multi-modal large language and vision models](https://arxiv.org/abs/2404.01322). Carolan, K., Fennelly, L., Smeaton, A.F., arXiv preprint (2024)
- [Allava: Harnessing gpt4v-synthesized data for a lite vision-language model](https://arxiv.org/abs/2402.11684). Chen, G.H., Chen, S., Zhang, R., Chen, J., Wu, X., Zhang, Z., Chen, Z., Li, J., Wan, X., Wang, B., arXiv preprint (2024)
- [How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites](https://arxiv.org/abs/2404.16821). Chen, Z., Wang, W., Tian, H., Ye, S., Gao, Z., Cui, E., Tong, W., Hu, K., Luo, J., Ma, Z., et al., arXiv preprint (2024)
- [Aigiqa-20k: A large database for ai-generated image quality assessment](https://arxiv.org/abs/2404.03407). Li, C., Kou, T., Gao, Y., Cao, Y., Sun, W., Zhang, Z., Zhou, Y., Zhang, Z., Zhang, W., Wu, H., et al., arXiv preprint (2024)
- [Clip with quality captions: A strong pretraining for vision tasks](https://arxiv.org/abs/2405.08911). Vasu, P.K.A., Pouransari, H., Faghri, F., Tuzel, O., arXiv preprint (2024)
- [Do llms understand visual anomalies? uncovering llm capabilities in zero-shot anomaly detection](https://arxiv.org/abs/2404.09654). Zhu, J., Cai, S., Deng, F., Wu, J., arXiv preprint (2024)
- [Imagereward: Learning and evaluating human preferences for text-to-image generation](https://papers.nips.cc/paper/2024/file/2e90ed12c4e40b6421e20e55db94d36d-Paper.pdf). Xu, J., Liu, X., Wu, Y., Tong, Y., Li, Q., Ding, M., Tang, J., Dong, Y., Advances in Neural Information Processing Systems (2024)
- [mplug-owl2: Revolutionizing multi-modal large language model with modality collaboration](https://arxiv.org/abs/2311.04257). Ye, Q., Xu, H., Ye, J., Yan, M., Liu, H., Qian, Q., Zhang, J., Huang, F., Zhou, J., arXiv preprint (2023)


