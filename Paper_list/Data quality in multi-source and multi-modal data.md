## Multi-source
- Markus, A., Hobson, B., Gunay, B., Bucking, S.: A framework for a
multi-source, data-driven building energy management toolkit. Energy and
Buildings 250, 111255 (2021) https://doi.org/10.1016/j.enbuild.
2021.111255
- Zhang, Z., Wu, W., Wu, D.: A multi-mode learning behavior real-time data
acquisition method based on data quality. In: 2021 2nd International Symposium on Computer Engineering and Intelligent Communications (IS-
CEIC), pp. 64–69 (2021). IEEE
- Sun, Y., Lu, T., Gu, N.: A method of electronic health data quality assessment: Enabling data provenance. In: 2017 IEEE 21st International Con-
ference on Computer Supported Cooperative Work in Design (CSCWD),
pp. 233–238 (2017). IEEE
- Ye, C., Wang, H., Zheng, K., Gao, J., Li, J.: Multi-source data repairing
powered by integrity constraints and source reliability. Information Sciences 507, 386–403 (2020)
- Fu, J., Li, S., Jiang, Y., Lin, K.-Y., Wu, W., Liu, Z.: Unitedhuman: Harnessing multi-source data for high-resolution human generation. In: Pro-
ceedings of the IEEE/CVF International Conference on Computer Vision,
pp. 7301–7311 (2023)
- Yang, K., Zhang, T., Kuang, Z., Xie, Q., Huang, J., Ananiadou, S.: Mentallama: interpretable mental health analysis on social media with large
language models. In: Proceedings of the ACM on Web Conference 2024,
pp. 4489–4500 (2024)
- Song, Z., Wang, Y., Zhang, W., Liu, K., Lyu, C., Song, D., Guo, Q., Yan, H., Lin, D., Chen, K., et al.: Alchemistcoder: Harmonizing and eliciting
code capability by hindsight tuning on multi-source data. arXiv preprint arXiv:2405.19265 (2024)
- Ge, L., Gao, J., Li, X., Zhang, A.: Multi-source deep learning for information trustworthiness estimation. In: Proceedings of the 19th ACM
SIGKDD International Conference on Knowledge Discovery and Data Mining. KDD ’13, pp. 766–774. Association for Computing Machinery, New
York, NY, USA (2013). https://doi.org/10.1145/2487575.2487612.

## Multi-modal
- Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al.: Learning transferable
visual models from natural language supervision. In: International Conference on Machine Learning, pp. 8748–8763 (2021). PMLR
- Vasu, P.K.A., Pouransari, H., Faghri, F., Tuzel, O.: Clip with quality captions: A strong pretraining for vision tasks. arXiv preprint
arXiv:2405.08911 (2024)
- Zhu, J., Cai, S., Deng, F., Wu, J.: Do llms understand visual anomalies?
uncovering llm capabilities in zero-shot anomaly detection. arXiv preprint
arXiv:2404.09654 (2024)
- Zhang, H., Li, X., Bing, L.: Video-llama: An instruction-tuned audio-visual
language model for video understanding. arXiv preprint arXiv:2306.02858
(2023)
- Wang, P., Sun, W., Zhang, Z., Jia, J., Jiang, Y., Zhang, Z., Min, X.,
Zhai, G.: Large multi-modality model assisted ai-generated image quality
assessment. arXiv preprint arXiv:2404.17762 (2024)
- Huang, Z., Zhang, Z., Lu, Y., Zha, Z.-J., Chen, Z., Guo, B.: Visual-critic: Making lmms perceive visual quality like humans. arXiv preprint
arXiv:2403.12806 (2024)
- Fang, Y., Sun, Z., Wu, T., Wang, J., Liu, Z., Wetzstein, G., Lin, D.: Make-it-real: Unleashing large multimodal model’s ability for painting 3d objects
with realistic materials. arXiv preprint arXiv:2404.16829 (2024)
- Li, Y., Zhang, Y., Wang, C., Zhong, Z., Chen, Y., Chu, R., Liu, S., Jia,
J.: Mini-gemini: Mining the potential of multi-modality vision language
models. arXiv preprint arXiv:2403.18814 (2024)
- Li, B., Zhang, P., Yang, J., Zhang, Y., Pu, F., Liu, Z.: Otterhd: A high-
resolution multi-modality model. arXiv preprint arXiv:2311.04219 (2023)
- Chen, G.H., Chen, S., Zhang, R., Chen, J., Wu, X., Zhang, Z., Chen, Z.,
Li, J., Wan, X., Wang, B.: Allava: Harnessing gpt4v-synthesized data for
a lite vision-language model. arXiv preprint arXiv:2402.11684 (2024)
- Zhu, D., Chen, J., Shen, X., Li, X., Elhoseiny, M.: Minigpt-4: Enhancing vision-language understanding with advanced large language models.
arXiv preprint arXiv:2304.10592 (2023)
- Carolan, K., Fennelly, L., Smeaton, A.F.: A review of multi-modal large
language and vision models. arXiv preprint arXiv:2404.01322 (2024)
- Chen, L., Li, J., Dong, X., Zhang, P., He, C., Wang, J., Zhao, F., Lin,
D.: Sharegpt4v: Improving large multi-modal models with better captions.
arXiv preprint arXiv:2311.12793 (2023)
- Chen, Z., Wang, W., Tian, H., Ye, S., Gao, Z., Cui, E., Tong, W., Hu,
K., Luo, J., Ma, Z., et al.: How far are we to gpt-4v? closing the gap
to commercial multimodal models with open-source suites. arXiv preprint
arXiv:2404.16821 (2024)
- Mehta, S., Deichler, A., O’Regan, J., Mo ̈ell, B., Beskow, J., Henter, G.E.,
Alexanderson, S.: Fake it to make it: Using synthetic data to remedy
the data shortage in joint multimodal speech-and-gesture synthesis. arXiv
preprint arXiv:2404.19622 (2024)
- Li, H., Zou, Y., Wang, Y., Majumder, O., Xie, Y., Manmatha, R., Swaminathan, A., Tu, Z., Ermon, S., Soatto, S.: On the scalability of diffusion-
based text-to-image generation. arXiv preprint arXiv:2404.02883 (2024)
Lecture Notes in Computer Science: Authors’ Instructions 43
- Hu, Y., Liu, B., Kasai, J., Wang, Y., Ostendorf, M., Krishna, R., Smith,
N.A.: Tifa: Accurate and interpretable text-to-image faithfulness evaluation with question answering. In: Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 20406–20417 (2023)
- Xu, J., Liu, X., Wu, Y., Tong, Y., Li, Q., Ding, M., Tang, J., Dong, Y.: Imagereward: Learning and evaluating human preferences for text-to-image
generation. Advances in Neural Information Processing Systems 36 (2024)
- Wu, X., Hao, Y., Sun, K., Chen, Y., Zhu, F., Zhao, R., Li, H.: Human preference score v2: A solid benchmark for evaluating human preferences
of text-to-image synthesis. arXiv preprint arXiv:2306.09341 (2023)
- Ye, Q., Xu, H., Ye, J., Yan, M., Liu, H., Qian, Q., Zhang, J., Huang, F.,
Zhou, J.: mplug-owl2: Revolutionizing multi-modal large language model with modality collaboration. arXiv preprint arXiv:2311.04257 (2023)
- Li, C., Kou, T., Gao, Y., Cao, Y., Sun, W., Zhang, Z., Zhou, Y., Zhang, Z.,
Zhang, W., Wu, H., et al.: Aigiqa-20k: A large database for ai-generated image quality assessment. arXiv preprint arXiv:2404.03407 (2024)
- Li, C., Zhang, Z., Wu, H., Sun, W., Min, X., Liu, X., Zhai, G., Lin, W.:
Agiqa-3k: An open database for ai-generated image quality assessment. IEEE Transactions on Circuits and Systems for Video Technology (2023)

