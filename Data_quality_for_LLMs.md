- [Rejeleene, R., Xu, X., & Talburt, J. (2024). Towards trustable language models: Investigating information quality of large language models. *arXiv*.](https://arxiv.org/abs/2401.13086)

- [Lipizzi, C. (2024). Tell me the truth: A system to measure the trustworthiness of Large Language Models. *arXiv*.](https://arxiv.org/abs/2403.04964)

- [Zhang, H., Dong, Y., Xiao, C., & Oyamada, M. (2024, November). Jellyfish: Instruction-tuning local large language models for data preprocessing. In *Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing* (pp. 8754–8782).](https://aclanthology.org/2024.emnlp-main.759)

- [Jaimovitch-López, G., Ferri, C., Hernández-Orallo, J., Martínez-Plumed, F., & Ramírez-Quintana, M. J. (2023). Can language models automate data wrangling? *Machine Learning, 112*(6), 2053–2082.](https://link.springer.com/article/10.1007/s10994-023-06423-7)

- [Zhang, H., Dong, Y., Xiao, C., & Oyamada, M. (2023). Large language models as data preprocessors. *arXiv*.](https://arxiv.org/abs/2308.16361)

- [Fang, X., Xu, W., Tan, F. A., Zhang, J., Hu, Z., Qi, Y., ... & Faloutsos, C. (2024). Large Language Models (LLMs) on Tabular Data: Prediction, Generation, and Understanding—A Survey. *arXiv*.](https://arxiv.org/abs/2402.17944)

- [Han, C. (2024). How can LLM help data quality management? *LinkedIn*.](https://www.linkedin.com/pulse/how-can-llm-helps-data-quality-management-chris-han-6ue6c/)

- [Yu, X., Zhang, Z., Niu, F., Hu, X., Xia, X., & Grundy, J. (2024, October). What makes a high-quality training dataset for large language models: A practitioners’ perspective. In *Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering* (pp. 656–668).](https://dl.acm.org/doi/10.1145/3647631)

- [MacMaster, S., & Sinistore, J. (2024). Testing the use of a large language model (LLM) for performing data quality assessment. *The International Journal of Life Cycle Assessment*, 1–12.](https://link.springer.com/article/10.1007/s11367-024-02263-3)

- [Li, M., Zhang, Y., Li, Z., Chen, J., Chen, L., Cheng, N., ... & Xiao, J. (2023). From quantity to quality: Boosting LLM performance with self-guided data selection for instruction tuning. *arXiv*.](https://arxiv.org/abs/2308.12032)

- [Saranathan, G., Alam, M. P., Lim, J., Bhattacharya, S., Wong, S. Y., Foltin, M., & Xu, C. (2024). Dele: Data efficient LLM evaluation. In *ICLR 2024 Workshop on Navigating and Addressing Data Problems for Foundation Models*.](https://openreview.net/forum?id=foundationdata24-16)

- [Zhou, X., Zhao, X., & Li, G. (2024). LLM-enhanced data management. *arXiv*.](https://arxiv.org/abs/2402.02643)

- [Vosoughi, A., Emmanouilidou, D., & Gamper, H. (2025). Quality over quantity? LLM-based curation for a data-efficient audio-video foundation model. *arXiv*.](https://arxiv.org/abs/2503.09205)

- [Pang, J., Wei, J., Shah, A. P., Zhu, Z., Wang, Y., Qian, C., ... & Wei, W. (2024). Improving data efficiency via curating LLM-driven rating systems. *arXiv*.](https://arxiv.org/abs/2410.10877)

- [Li, L., Fang, L., & Torvik, V. I. (2024). AutoDCWorkflow: LLM-based data cleaning workflow auto-generation and benchmark. *arXiv*.](https://arxiv.org/abs/2412.06724)

- [Li, G., Zhou, X., & Zhao, X. (2024). LLM for data management. *Proceedings of the VLDB Endowment, 17*(12), 4213–4216.](https://dl.acm.org/doi/10.14778/3633010.3633034)

- [Wang, J. T., Wu, T., Song, D., Mittal, P., & Jia, R. (2024). GREATS: Online selection of high-quality data for LLM training in every iteration. *Advances in Neural Information Processing Systems, 37*, 131197–131223.](https://proceedings.neurips.cc/paper_files/paper/2024/file/e3f032dc4e986e54b32c18493f6208d4-Paper-Conference.pdf)

- [Eun, H. J., & Yong, G. G. (2024). A study of how LLM-based generative AI response data quality affects impact on job satisfaction. *Convergence Security Journal, 24*(3), 117–129.](https://www.dbpia.co.kr/journal/articleDetail?nodeId=NODE11728568)

- [Nananukul, N., & Kekriwal, M. (2024, September). Balancing efficiency and quality in LLM-based entity resolution on structured data. In *International Conference on Advances in Social Networks Analysis and Mining* (pp. 278–293). Cham: Springer Nature Switzerland.](https://link.springer.com/chapter/10.1007/978-3-031-56017-1_21)

- [Team, L., Cai, W., Cao, Y., Chen, C., Chen, C., Chen, S., ... & Zhou, J. (2025). Every sample matters: Leveraging mixture-of-experts and high-quality data for efficient and accurate code LLM. *arXiv*.](https://arxiv.org/abs/2503.17793)
