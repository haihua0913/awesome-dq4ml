# Autonomous Vehicles (AVs)
Alternative names: Driverless Cars, Automated Vehicles, Autonomous Driving, Self-Driving, Robotic Cars, Unmanned Vehicles, Unmanned Vehicles

# Datasets

- [Zenseact Open Dataset: A Large-Scale and Diverse Multimodal Dataset for Autonomous Driving](http://arxiv.org/abs/2305.02008)  
  Alibeigi, M., Ljungbergh, W., Tonderski, A., Hess, G., Lilja, A., LindstrÃ¶m, C., Motorniuk, D., Fu, J., Widahl, J., & Petersson, C. (2023). *arXiv*.  

  ðŸ”¹ Introduces a large-scale and diverse multimodal dataset designed to enhance perception and prediction tasks in autonomous driving.

- [MUSES: The Multi-Sensor Semantic Perception Dataset for Driving Under Uncertainty](http://arxiv.org/abs/2401.12761)  
  BrÃ¶dermann, T., BrÃ¼ggemann, D., Sakaridis, C., Ta, K., Liagouris, O., Corkill, J., & Van Gool, L. (2024). *arXiv*.  

  ðŸ”¹ Presents a multi-sensor dataset focused on perception under uncertainty for autonomous driving applications.

- [Boreas: A Multi-Season Autonomous Driving Dataset](https://doi.org/10.1177/02783649231160195)  
  Burnett, K., Yoon, D. J., Wu, Y., Li, A. Z., Zhang, H., Lu, S., Qian, J., et al. (2023). *The International Journal of Robotics Research, 42*(1â€“2), 33â€“42.  

  ðŸ”¹ Provides an autonomous driving dataset covering multiple seasons to analyze sensor performance in different environmental conditions.

- [TimePillars: Temporally-Recurrent 3D LiDAR Object Detection](http://arxiv.org/abs/2312.17260)  
  Calvo, E. L., Taveira, B., Kahl, F., Gustafsson, N., Larsson, J., & Tonderski, A. (2023). *arXiv*.  

  ðŸ”¹ Proposes a temporally-recurrent 3D LiDAR object detection method, leveraging temporal context to improve accuracy.

- [Weather and Light Level Classification for Autonomous Driving: Dataset, Baseline and Active Learning](https://doi.org/10.1109/ITSC48978.2021.9564689)  
  Dhananjaya, M. M., Ravi Kumar, V., & Yogamani, S. (2021). *2021 IEEE International Intelligent Transportation Systems Conference (ITSC)*, 2816â€“2821.  

  ðŸ”¹ Introduces a dataset and baseline methods for classifying weather and lighting conditions in autonomous driving.

- [L-RadSet: A Long-Range Multimodal Dataset with 4D Radar for Autonomous Driving and Its Application](https://doi.org/10.1109/TIV.2024.3424942)  
  Li, J., Wang, Y., Yang, L., Lin, J., Kang, G., Shi, Z., Chen, Y., Jin, Y., & Akiyama, K. (2024). *IEEE Transactions on Intelligent Vehicles*, 1â€“16.  

  ðŸ”¹ Presents a multimodal dataset incorporating 4D radar for improving perception in long-range autonomous driving scenarios.

- [Multiagent Multitraversal Multimodal Self-Driving: Open MARS Dataset]  
  Li, Y., Li, Z., Chen, N., Gong, M., Lyu, Z., Wang, Z., Jiang, P., & Feng, C.  

  ðŸ”¹ Introduces the Open MARS dataset, focusing on multi-agent interactions and multimodal data for self-driving applications.

- [NeuroNCAP: Photorealistic Closed-Loop Safety Testing for Autonomous Driving](http://arxiv.org/abs/2404.07762)  
  Ljungbergh, W., Tonderski, A., Johnander, J., Caesar, H., Ã…strÃ¶m, K., Felsberg, M., & Petersson, C. (2024). *arXiv*.  

  ðŸ”¹ Proposes a photorealistic simulation framework for safety testing in autonomous driving using closed-loop evaluation.



## Dataset Survey

- [Deep Multi-Modal Object Detection and Semantic Segmentation for Autonomous Driving: Datasets, Methods, and Challenges]  
  (2020). *arXiv preprint*.  

  ðŸ”¹ Provides a comprehensive overview of datasets, methodologies, and challenges associated with deep multi-modal object detection and semantic segmentation in autonomous driving.

- [Advances in Autonomous Vehicle Testing: The State of the Art and Future Outlook on Driving Datasets, Simulators, and Proving Grounds](https://doi.org/10.22541/au.172446843.37817639/v1)  
  Guo, A., Li, Y., Huang, J., Li, B., Na, X., Lv, C., Chen, L., Li, L., & Wang, F.-Y. (2024). *arXiv preprint*.  

  ðŸ”¹ Reviews current advancements in autonomous vehicle testing, focusing on driving datasets, simulation platforms, and proving grounds, and discusses future research directions.



## Synthesized datasets

- [DriveDiTFit: Fine-tuning diffusion transformers for autonomous driving](http://arxiv.org/abs/2407.15661)  
  Tu, J., Ji, W., Zhao, H., Zhang, C., Zimmermann, R., & Qian, H. (2024). *arXiv preprint*.  

  ðŸ”¹ Introduces DriveDiTFit, a fine-tuning framework leveraging diffusion transformers to improve autonomous driving perception and planning.

- [SurfelGAN: Synthesizing realistic sensor data for autonomous driving](http://arxiv.org/abs/2005.03844)  
  Yang, Z., Chai, Y., Anguelov, D., Zhou, Y., Sun, P., Erhan, D., Rafferty, S., & Kretzschmar, H. (2020). *arXiv preprint*.  

  ðŸ”¹ Proposes SurfelGAN, a generative model designed to synthesize realistic sensor data for training and validating autonomous driving perception systems.



# Data Fusion

- [DeepInteraction++: Multi-Modality Interaction for Autonomous Driving](http://arxiv.org/abs/2408.05075)  
  Yang, Z., Song, N., Li, W., Zhu, X., Zhang, L., & Torr, P. H. S. (2024). *arXiv preprint*.  

  ðŸ”¹ Proposes DeepInteraction++, a novel multi-modal interaction strategy for improving autonomous driving perception by preserving modality-specific representations.

- [Multimodal Fusion on Low-Quality Data: A Comprehensive Survey](http://arxiv.org/abs/2404.18947)  
  Zhang, Q., Wei, Y., Han, Z., Fu, H., Peng, X., Deng, C., Hu, Q., Xu, C., Wen, J., Hu, D., & Zhang, C. (2024). *arXiv preprint*.  

  ðŸ”¹ Surveys multimodal fusion techniques for low-quality data, categorizing challenges such as noisy, incomplete, imbalanced, and quality-varying data.

- [Deep Multimodal Data Fusion](https://dl.acm.org/doi/10.1145/3649447)  
  Zhao, F., Zhang, C., & Geng, B. (2024). *ACM Computing Surveys, 56*(9), 1â€“36. https://doi.org/10.1145/3649447  

  ðŸ”¹ Introduces a fine-grained taxonomy for multimodal fusion techniques, covering encoder-decoder models, attention mechanisms, GNNs, and generative methods.

- [MST-GAT: A Multimodal Spatial-Temporal Graph Attention Network for Time Series Anomaly Detection](http://arxiv.org/abs/2310.11169)  
  Ding, C., Sun, S., & Zhao, J. (2023). *Information Fusion, 89*, 527â€“536. https://doi.org/10.1016/j.inffus.2022.08.011  

  ðŸ”¹ Proposes MST-GAT, a graph attention network designed for detecting anomalies in multimodal time-series data by capturing intra- and inter-modal correlations.

- [Object Detection in Adverse Weather for Autonomous Vehicles Based on Sensor Fusion and Incremental Learning](https://ieeexplore.ieee.org/document/10715515/)  
  Su, H., Gao, H., Wang, X., Fang, X., Liu, Q., Huang, G., Li, X., & Cao, Q. (2024). *IEEE Transactions on Instrumentation and Measurement, 1â€“1.* https://doi.org/10.1109/TIM.2024.3472860  

  ðŸ”¹ Introduces an object detection approach leveraging sensor fusion and incremental learning to enhance robustness in adverse weather conditions.

- [Multi-Modal 3D Object Detection in Autonomous Driving: A Survey and Taxonomy](https://ieeexplore.ieee.org/document/10093116/)  
  Wang, L., Zhang, X., Song, Z., Bi, J., Zhang, G., Wei, H., Tang, L., Yang, L., Li, J., Jia, C., & Zhao, L. (2023). *IEEE Transactions on Intelligent Vehicles, 8*(7), 3781â€“3798. https://doi.org/10.1109/TIV.2023.3264658  

  ðŸ”¹ Provides a taxonomy of multi-modal 3D object detection approaches in autonomous driving, discussing sensor fusion strategies and performance benchmarks.

- [Eliminating Cross-Modal Conflicts in BEV Space for LiDAR-Camera 3D Object Detection](http://arxiv.org/abs/2403.07372)  
  Fu, J., Gao, C., Wang, Z., Yang, L., Wang, X., Mu, B., & Liu, S. (2024). *arXiv preprint*.  

  ðŸ”¹ Proposes ECFusion, a method to mitigate cross-modal conflicts in birdâ€™s-eye view (BEV) representations for LiDAR-camera fusion in 3D object detection.

- [Explore the LiDAR-Camera Dynamic Adjustment Fusion for 3D Object Detection](http://arxiv.org/abs/2407.15334)  
  Yang, Y., Gao, X., Wang, T., Hao, X., Shi, Y., Tan, X., Ye, X., & Wang, J. (2024). *arXiv preprint*.  

  ðŸ”¹ Introduces a dynamic adjustment technique to align modality distributions, improving fusion performance for LiDAR-camera-based 3D object detection.




# LLMs in AV

- [MME-RealWorld: Could Your Multimodal LLM Challenge High-Resolution Real-World Scenarios that are Difficult for Humans?](http://arxiv.org/abs/2408.13257)  
  Zhang, Y.-F., Zhang, H., Tian, H., Fu, C., Zhang, S., Wu, J., Li, F., Wang, K., Wen, Q., Zhang, Z., Wang, L., Jin, R., & Tan, T. (2024). *arXiv preprint*.  

  ðŸ”¹ Introduces MME-RealWorld, the largest manually annotated benchmark for evaluating multimodal LLMs in high-resolution real-world scenarios.

- [LingoQA: Video Question Answering for Autonomous Driving](http://arxiv.org/abs/2312.14115)  
  Marcu, A.-M., Chen, L., HÃ¼nermann, J., Karnsund, A., Hanotte, B., Chidananda, P., Nair, S., Badrinarayanan, V., Kendall, A., Shotton, J., Arani, E., & Sinavski, O. (2024). *arXiv preprint*.  

  ðŸ”¹ Introduces LingoQA, a benchmark for evaluating video question-answering (QA) in autonomous driving.

- [LaMPilot: An Open Benchmark Dataset for Autonomous Driving with Language Model Programs](https://arxiv.org/abs/2312.09245)  
  Ma, Y., Cui, C., Cao, X., Ye, W., Liu, P., Lu, J., Abdelraouf, A., Gupta, R., Han, K., Bera, A., Rehg, J. M., & Wang, Z. (n.d.). *arXiv preprint*.  

  ðŸ”¹ Proposes LaMPilot, a dataset integrating language model programs for autonomous driving evaluation.

- [GPT-4 Enhanced Multimodal Grounding for Autonomous Driving: Leveraging Cross-Modal Attention with Large Language Models](https://doi.org/10.1016/j.commtr.2023.100116)  
  Liao, H., Shen, H., Li, Z., Wang, C., Li, G., Bie, Y., & Xu, C. (2024). *Communications in Transportation Research, 4*, 100116.  

  ðŸ”¹ Introduces a GPT-4-enhanced multimodal grounding model that improves visual grounding for AVs using cross-modal attention mechanisms.

- [GenFollower: Enhancing Car-Following Prediction with Large Language Models](http://arxiv.org/abs/2407.05611)  
  Chen, X., Peng, M., Tiu, P., Wu, Y., Chen, J., Zhu, M., & Zheng, X. (2024). *arXiv preprint*.  

  ðŸ”¹ Develops GenFollower, a zero-shot prompting approach using LLMs for car-following behavior prediction in autonomous driving.

- [A Survey on Multimodal Large Language Models for Autonomous Driving](https://ieeexplore.ieee.org/document/10495592/)  
  Cui, C., Ma, Y., Cao, X., Ye, W., Zhou, Y., Liang, K., Chen, J., Lu, J., Yang, Z., Liao, K.-D., Gao, T., Li, E., Tang, K., Cao, Z., Zhou, T., Liu, A., Yan, X., Mei, S., Cao, J., Wang, Z., & Zheng, C. (2024). *IEEE/CVF Winter Conference on Applications of Computer Vision Workshops (WACVW)*, 958â€“979.  

  ðŸ”¹ Provides a systematic survey on the role of multimodal large language models (MLLMs) in autonomous driving.

- [SurrealDriver: Designing LLM-Powered Generative Driver Agent Framework Based on Human Driversâ€™ Driving-Thinking Data](https://ieeexplore.ieee.org/document/10802229/)  
  Jin, Y., Yang, R., Yi, Z., Shen, X., Peng, H., Liu, X., Qin, J., Li, J., Xie, J., Gao, P., Zhou, G., & Gong, J. (2024). *IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)*, 966â€“971.  

  ðŸ”¹ Proposes SurrealDriver, an LLM-driven driver agent trained on human driversâ€™ decision-making data.

- [Driving Style Alignment for LLM-Powered Driver Agent](http://arxiv.org/abs/2403.11368)  
  Yang, R., Zhang, X., Fernandez-Laaksonen, A., Ding, X., & Gong, J. (2024). *arXiv preprint*.  

  ðŸ”¹ Develops a framework to align LLM-powered driver agents with human driving styles using natural language feedback.

- [What Limits LLM-Based Human Simulation: LLMs or Our Design?](http://arxiv.org/abs/2501.08579)  
  Wang, Q., Wu, J., Tang, Z., Luo, B., Chen, N., Chen, W., & He, B. (2025). *arXiv preprint*.  

  ðŸ”¹ Examines the limitations of LLM-based human simulation, addressing challenges in both model capabilities and framework design.

- [Text-to-Drive: Diverse Driving Behavior Synthesis via Large Language Models](http://arxiv.org/abs/2406.04300)  
  Nguyen, P., Wang, T.-H., Hong, Z.-W., Karaman, S., & Rus, D. (2024). *arXiv preprint*.  

  ðŸ”¹ Introduces Text-to-Drive, an LLM-powered system for generating diverse and realistic driving behaviors.

- [DriVLMe: Enhancing LLM-Based Autonomous Driving Agents with Embodied and Social Experiences](http://arxiv.org/abs/2406.03008)  
  Huang, Y., Sansom, J., Ma, Z., Gervits, F., & Chai, J. (2024). *arXiv preprint*.  

  ðŸ”¹ Proposes DriVLMe, an LLM-based agent integrating real-world interactions and social experiences for autonomous driving.

- [DriveMLM: Aligning Multi-Modal Large Language Models with Behavioral Planning States for Autonomous Driving](http://arxiv.org/abs/2312.09245)  
  Wang, W., Xie, J., Hu, C., Zou, H., Fan, J., Tong, W., Wen, Y., Wu, S., Deng, H., Li, Z., Tian, H., Lu, L., Zhu, X., Wang, X., Qiao, Y., & Dai, J. (2023). *arXiv preprint*.  

  ðŸ”¹ Introduces DriveMLM, a framework that aligns MLLMs with behavioral planning states for autonomous driving.

- [LidarCLIP or: How I Learned to Talk to Point Clouds](http://arxiv.org/abs/2212.06858)  
  Hess, G., Tonderski, A., Petersson, C., Ã…strÃ¶m, K., & Svensson, L. (2023). *arXiv preprint*.  

  ðŸ”¹ Proposes LidarCLIP, a method for mapping point cloud data to CLIP embedding space to enable text-based interaction with 3D LiDAR data.




# Quality and Metrics

- [VRSO: Visual-Centric Reconstruction for Static Object Annotation](http://arxiv.org/abs/2403.15026)  
  Yu, C., Cai, Y., Zhang, J., Kong, H., Sui, W., & Yang, C. (2024). *arXiv preprint*.  

  ðŸ”¹ Proposes a visual-centric approach for static object annotation, reducing the need for manual labeling in 3D object detection tasks.

- [Evaluation of Dataset Distribution and Label Quality for Autonomous Driving System](https://ieeexplore.ieee.org/document/9742177/)  
  Li, S., Fan, Y., Ma, Y., & Pan, Y. (2021). *IEEE International Conference on Software Quality, Reliability and Security Companion (QRS-C)*, 196â€“200.  

  ðŸ”¹ Evaluates dataset distribution and label quality issues in datasets used for autonomous driving systems.

- [Quality and Relevance Metrics for Selection of Multimodal Pretraining Data](https://ieeexplore.ieee.org/document/9150898/)  
  Rao, R., Rao, S., Nouri, E., Dey, D., Celikyilmaz, A., & Dolan, B. (2020). *IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)*, 4109â€“4116.  

  ðŸ”¹ Proposes dataset selection metrics to improve the quality and relevance of multimodal pretraining data.

- [Sensor Data Quality: A Systematic Review](https://journalofbigdata.springeropen.com/articles/10.1186/s40537-020-0285-1)  
  Teh, H. Y., Kempa-Liehr, A. W., & Wang, K. I.-K. (2020). *Journal of Big Data, 7*(1), 11.  

  ðŸ”¹ Reviews types of sensor data errors, error detection methods, and correction techniques in IoT and sensor networks.

- [Multi-Modal Data-Efficient 3D Scene Understanding for Autonomous Driving](http://arxiv.org/abs/2405.05258)  
  Kong, L., Xu, X., Ren, J., Zhang, W., Pan, L., Chen, K., Ooi, W. T., & Liu, Z. (2024). *arXiv preprint*.  

  ðŸ”¹ Introduces a semi-supervised framework to enhance 3D scene understanding by leveraging multi-modal sensor data.

- [The METRIC-Framework for Assessing Data Quality for Trustworthy AI in Medicine: A Systematic Review](https://www.nature.com/articles/s41746-024-01196-4)  
  Schwabe, D., Becker, K., Seyferth, M., KlaÃŸ, A., & Schaeffter, T. (2024). *NPJ Digital Medicine, 7*(1), 203.  

  ðŸ”¹ Proposes a data quality framework tailored for medical AI, identifying 15 key dimensions to assess training datasets.

- [A Systematic Review of Data Quality in CPS and IoT for Industry 4.0](https://dl.acm.org/doi/10.1145/3593043)  
  Goknil, A., Nguyen, P., Sen, S., Politaki, D., Niavis, H., Pedersen, K. J., Suyuthi, A., Anand, A., & Ziegenbein, A. (2023). *ACM Computing Surveys, 55*(14), 1â€“38.  

  ðŸ”¹ Summarizes research on data quality challenges, techniques, and best practices for cyber-physical systems and IoT in Industry 4.0.

- [Image-Guided Outdoor LiDAR Perception Quality Assessment for Autonomous Driving](https://ieeexplore.ieee.org/document/10726865/)  
  Zhang, C., & Eskandarian, A. (2024). *IEEE Transactions on Intelligent Vehicles*, 1â€“12.  

  ðŸ”¹ Proposes an image-guided approach to assess LiDAR perception quality in outdoor autonomous driving environments.
  
- [Light the Night: A Multi-Condition Diffusion Framework for Unpaired Low-Light Enhancement in Autonomous Driving](https://openaccess.thecvf.com/content/CVPR2024/html/Li_Light_the_Night_A_Multi-Condition_Diffusion_Framework_for_Unpaired_Low-Light_CVPR_2024_paper.html)  
  Li, J., Li, B., Tu, Z., Liu, X., Guo, Q., Juefei-Xu, F., ... & Yu, H. (2024). *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, 15205â€“15215.  

  ðŸ”¹ Proposes a diffusion-based framework for enhancing low-light images in autonomous driving without requiring paired data.

- [Traffic Context Aware Data Augmentation for Rare Object Detection in Autonomous Driving](https://ieeexplore.ieee.org/document/9811734)  
  Li, N., Song, F., Zhang, Y., Liang, P., & Cheng, E. (2022, May). *Proceedings of the IEEE International Conference on Robotics and Automation (ICRA)*, 4548â€“4554.  

  ðŸ”¹ Introduces a traffic-context-aware data augmentation strategy to improve rare object detection performance in autonomous driving.

- [A Survey on Deep Learning Approaches for Data Integration in Autonomous Driving Systems](https://arxiv.org/abs/2306.11740)  
  Zhu, X., Wang, L., Zhou, C., Cao, X., Gong, Y., & Chen, L. (2023). *arXiv preprint*.  

  ðŸ”¹ Reviews deep learning techniques for integrating multi-modal sensor data in autonomous driving systems, discussing challenges and future directions.



## Redundancy

- [Quality Assessment of Image Dataset for Autonomous Driving](https://ieeexplore.ieee.org/document/10355734/)  
  Li, X., Zhang, Y., Shi, Y., Zhu, H., Hu, J., & Peng, L. (2023). *IEEE International Conference on Imaging Systems and Techniques (IST)*, 1â€“6.  

  ðŸ”¹ Proposes a framework for assessing the quality of image datasets used in autonomous driving to ensure robust perception models.

- [ApolloCar3D: A Large 3D Car Instance Understanding Benchmark for Autonomous Driving](https://ieeexplore.ieee.org/document/8954083/)  
  Song, X., Wang, P., Zhou, D., Zhu, R., Guan, C., Dai, Y., Su, H., Li, H., & Yang, R. (2019). *IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)*, 5447â€“5457.  

  ðŸ”¹ Introduces ApolloCar3D, a large-scale 3D car instance dataset for understanding vehicle properties in autonomous driving.

- [3D Object Detection for Autonomous Driving: A Survey](https://doi.org/10.1016/j.patcog.2022.108796)  
  Qian, R., Lai, X., & Li, X. (2022). *Pattern Recognition, 130*, 108796.  

  ðŸ”¹ Surveys existing approaches for 3D object detection in autonomous driving, covering sensors, datasets, performance metrics, and recent advancements.

- [Exploring Inherent Sensor Redundancy for Automotive Anomaly Detection](https://ieeexplore.ieee.org/document/9218557/)  
  He, T., Zhang, L., Kong, F., & Salekin, A. (2020). *ACM/IEEE Design Automation Conference (DAC)*, 1â€“6.  

  ðŸ”¹ Proposes a redundancy-based anomaly detection method for automotive sensors to enhance system safety.

- [Multi-Target Detection Based on Multi-Sensor Redundancy and Dynamic Weight Distribution for Driverless Cars](https://ieeexplore.ieee.org/document/9446002/)  
  Liu, Q., Zhou, W., Zhang, Y., & Fei, X. (2021). *International Conference on Communications, Information System and Computer Engineering (CISCE)*, 229â€“234.  

  ðŸ”¹ Introduces a multi-sensor redundancy-based detection approach for improving target recognition in autonomous vehicles.

- [Real-Time Dynamic Object Detection for Autonomous Driving Using Prior 3D Maps](https://link.springer.com/10.1007/978-3-030-11021-5_35)  
  Kiran, B. R., RoldÃ£o, L., Irastorza, B., Verastegui, R., SÃ¼ss, S., Yogamani, S., Talpaert, V., Lepoutre, A., & Trehard, G. (2019). *Computer Vision â€“ ECCV 2018 Workshops*, 567â€“582.  

  ðŸ”¹ Proposes a real-time object detection method utilizing prior 3D maps to enhance efficiency in autonomous driving.

- [Cross-Modal Matching CNN for Autonomous Driving Sensor Data Monitoring](https://ieeexplore.ieee.org/document/9607579/)  
  Chen, Y., Liu, F., & Pei, K. (2021). *IEEE/CVF International Conference on Computer Vision Workshops (ICCVW)*, 3103â€“3112.  

  ðŸ”¹ Develops a cross-modal CNN for detecting sensor faults and assessing data quality in autonomous driving.

- [The Why, When, and How to Use Active Learning in Large-Data-Driven 3D Object Detection for Safe Autonomous Driving: An Empirical Exploration](http://arxiv.org/abs/2401.16634)  
  Greer, R., Antoniussen, B., Andersen, M. V., MÃ¸gelmose, A., & Trivedi, M. M. (2024). *arXiv preprint*.  

  ðŸ”¹ Investigates entropy-based active learning techniques for improving 3D object detection in autonomous driving.

- [An Advanced LiDAR Point Cloud Sequence Coding Scheme for Autonomous Driving](https://dl.acm.org/doi/10.1145/3394171.3413537)  
  Sun, X., Wang, S., Wang, M., Cheng, S. S., & Liu, M. (2020). *Proceedings of the 28th ACM International Conference on Multimedia*, 2793â€“2801.  

  ðŸ”¹ Proposes a LiDAR point cloud coding scheme to enhance storage efficiency while maintaining high data fidelity.

- [Sensor Data Validation and Driving Safety in Autonomous Driving Systems](http://arxiv.org/abs/2203.16130)  
  Zhang, J. (2022). *arXiv preprint*.  

  ðŸ”¹ Examines attack detection methods for onboard sensors and their impact on autonomous vehicle safety.

- [Active Learning with Data Augmentation Under Small vs Large Dataset Regimes for Semantic-KITTI Dataset](https://link.springer.com/10.1007/978-3-031-45725-8_13)  
  Duong, N. P. A., Almin, A., LemariÃ©, L., & Kiran, B. R. (2023). *Computer Vision, Imaging and Computer Graphics Theory and Applications*, 268â€“280.  

  ðŸ”¹ Studies the impact of active learning and data augmentation strategies under varying dataset sizes for semantic segmentation.

- [Eliminating Cross-Modal Conflicts in BEV Space for LiDAR-Camera 3D Object Detection](http://arxiv.org/abs/2403.07372)  
  Fu, J., Gao, C., Wang, Z., Yang, L., Wang, X., Mu, B., & Liu, S. (2024). *arXiv preprint*.  

  ðŸ”¹ Proposes a method to mitigate cross-modal conflicts in bird's-eye view (BEV) space for LiDAR-camera 3D object detection.

- [Deep Learning with HM-VGG: AI Strategies for Multi-Modal Image Analysis](http://arxiv.org/abs/2410.24046)  
  Du, J., Cang, Y., Zhou, T., Hu, J., & He, W. (2024). *arXiv preprint*.  

  ðŸ”¹ Introduces HM-VGG, a hybrid multi-modal deep learning model optimized for medical image analysis.

- [AlterMOMA: Fusion Redundancy Pruning for Camera-LiDAR Fusion Models with Alternative Modality Masking](http://arxiv.org/abs/2409.17728)  
  Sun, S., Lu, Y., Liu, N., Jiang, B., Chen, J., & Zhang, Y. (2024). *arXiv preprint*.  

  ðŸ”¹ Develops a pruning method to remove redundant features in camera-LiDAR fusion models for autonomous driving.

- [Key Safety Design Overview in AI-Driven Autonomous Vehicles](http://arxiv.org/abs/2412.08862)  
  Vyas, V., & Xu, Z. (2024). *arXiv preprint*.  

  ðŸ”¹ Discusses safety architecture, mitigation strategies, and failure analysis in AI-driven autonomous vehicle systems.

- [Survey: Exploiting Data Redundancy for Optimization of Deep Learning](https://dl.acm.org/doi/10.1145/3564663)  
  Chen, J., Niu, W., Ren, B., Wang, Y., & Shen, X. (2023). *ACM Computing Surveys, 55*(10), 1â€“38.  

  ðŸ”¹ Provides a systematic review of techniques that leverage data redundancy to improve deep learning efficiency.

- [Multimodal Informative ViT: Information Aggregation and Distribution for Hyperspectral and LiDAR Classification](https://ieeexplore.ieee.org/document/10464367/)  
  Zhang, J., Lei, J., Xie, W., Yang, G., Li, D., & Li, Y. (2024). *IEEE Transactions on Circuits and Systems for Video Technology, 34*(8), 7643â€“7656.  

  ðŸ”¹ Introduces a vision transformer (ViT)-based model for multimodal classification of hyperspectral and LiDAR data.

- [Learning Modality-Complementary and Eliminating-Redundancy Representations with Multi-Task Learning for Multimodal Sentiment Analysis](https://ieeexplore.ieee.org/document/10650083/)  
  Zhao, X., Miao, X., Xu, X., Liu, Y., & Cao, Y. (2024). *International Joint Conference on Neural Networks (IJCNN)*, 1â€“8.  

  ðŸ”¹ Proposes a multi-task learning framework to improve redundancy handling in multimodal sentiment analysis.








